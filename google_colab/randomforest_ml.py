# -*- coding: utf-8 -*-
"""RandomForest_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YHC8VBq3toRnL7bjpsE2FPcDgFhR8Buq
"""

from google.colab import files
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.utils.multiclass import unique_labels
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight
from micromlgen import port
from imblearn.over_sampling import SMOTE
import joblib
import io

def compute_acc(row):
    features = {}

    acc_x = [row[f'acc_x{i}'] for i in range (1,101)]
    acc_y = [row[f'acc_y{i}'] for i in range (1,101)]
    acc_z = [row[f'acc_z{i}'] for i in range (1,101)]

    acc_x = np.array(acc_x)
    acc_y = np.array(acc_y)
    acc_z = np.array(acc_z)

    for axis_name, axis_data in [('x',acc_x),('y',acc_y),('z',acc_z)]:
        features[f'acc_{axis_name}_mean'] = np.mean(axis_data)
        features[f'acc_{axis_name}_std'] = np.std(axis_data)
        features[f'acc_{axis_name}_min'] = np.min(axis_data)
        features[f'acc_{axis_name}_max'] = np.max(axis_data)
        features[f'acc_{axis_name}_range'] = np.ptp(axis_data)
        features[f'acc_{axis_name}_rms'] = np.sqrt(np.mean(axis_data**2))

        #Slope Computation through Linear Regression Graph
        if len(axis_data) > 1:
            timesteps = np.arange(len(axis_data))
            slope, _ = np.polyfit(timesteps,axis_data, 1)
            features[f'acc_{axis_name}_slope'] = slope
        else:
            features[f'acc_{axis_name}_slope'] = 0

    #Magnitude of overall axes
    magnitude = np.sqrt(acc_x**2 + acc_y**2 + acc_z**2) #Sum square for each array elements
    #Compute mean,stddev and max for each elements
    features['acc_magnitude_mean'] = np.mean(magnitude)
    features['acc_magnitude_std'] = np.std(magnitude)
    features['acc_magnitude_max'] = np.max(magnitude)

    return features

def compute_temporal(series, feature_prefix):
    features = {}

    features[f'{feature_prefix}_mean'] = np.mean(series)
    features[f'{feature_prefix}_std'] = np.std(series)
    features[f'{feature_prefix}_min'] = np.min(series)
    features[f'{feature_prefix}_max'] = np.max(series)

    #Slope computation for each prefix
    if len(series) > 1:
        timesteps = np.arange(len(series)).reshape(-1,1)
        model = LinearRegression()
        model.fit(timesteps, series)
        features[f'{feature_prefix}_slope'] = model.coef_[0] #y = ax + b "a is coefficient"
    else:
        features[f'{feature_prefix}_slope'] = 0

    #Rate of Change for each prefix
    if len(series) >= 2:
        features[f'{feature_prefix}_roc'] = series[-1] - series[0]
    else:
        features[f'{feature_prefix}_roc'] = 0

    #Z-score
    if features[f'{feature_prefix}_std'] > 0:
        features[f'{feature_prefix}_zscore'] = (series[-1] - features[f'{feature_prefix}_mean']) / features[f'{feature_prefix}_std']
    else:
        features[f'{feature_prefix}_zscore'] = 0

    return features

def create_dataset (df, window_size = 5):
    #Create required statistical data using csv file dataset
    #Main data processing

    processed_rows = []
    for i in range (len(df)):
        if i < window_size - 1:
            continue #Skip processing for windows < window_size
        window_indices = range(i - window_size + 1, i + 1)
        window_data = df.iloc[window_indices]

        features = {}
        temp_series = window_data['temp'].values
        light_series = window_data['light'].values

        temp_features = compute_temporal(temp_series, 'temp')
        light_features = compute_temporal(light_series, 'light')

        features.update(temp_features) #Update data in the dictionary
        features.update(light_features)

        current_row = df.iloc[i]
        acc_features = compute_acc(current_row)
        features.update(acc_features)

        #Append dictionary of every row into a list
        processed_rows.append(features)

    return pd.DataFrame(processed_rows)

def process_and_label(filename, label_name):
    print(f"Processing {filename} as {label_name}")
    df_raw = pd.read_csv(io.BytesIO(uploaded[filename]))

    df_features = create_dataset(df_raw, window_size=5)

    df_features['label'] = label_name
    df_features['source'] = filename
    return df_features

if __name__ == "__main__":
    uploaded = files.upload()

    file_mapping = [
        ("logged_data_normal.csv","normal"),
        ("logged_data_abnormal_vibration.csv", "abnormal vibration"),
        ("logged_data_heat.csv", "overheating"),
        ("logged_data_light_flickering.csv","light instability"),
        ("logged_data_impact.csv","impact_shock"),
        ("logged_data_tampering.csv","tampering"),
        ("logged_data_light_failure.csv","Light Failure")
    ]

    all_data_frames = []

    for fname, label in file_mapping:
        if fname in uploaded:
            df_part = process_and_label(fname, label)
            if not df_part.empty:
                all_data_frames.append(df_part)
        else:
            print(f"Warning: {fname} was not uploaded. Ignoring class '{label}'.")

    if not all_data_frames:
        print("No valid data found. Exiting.")
        exit()

    # Merge into one Master Dataset
    df_master = pd.concat(all_data_frames, ignore_index=True)

    # Shuffle the data (Crucial for training!)
    df_master = df_master.sample(frac=1, random_state=42).reset_index(drop=True)

    print(f"\nTotal Training Samples: {len(df_master)}")
    print("Class Distribution:")
    print(df_master['label'].value_counts())

    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(df_master['label'])

    X = df_master.drop(columns=['label', 'source'])
    y = y_encoded

    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    train_idx, test_idx = next(sss.split(X, y))

    X_train = X.iloc[train_idx]
    X_test  = X.iloc[test_idx]
    y_train = y[train_idx]
    y_test  = y[test_idx]

    # Data Augmentation (keep as DataFrame!)
    noise_level = 0.02
    rng = np.random.default_rng(42)

    X_noisy = X_train + rng.normal(0, noise_level, X_train.shape)

    X_train_aug = pd.concat([X_train, X_noisy], ignore_index=True)
    y_train_aug = np.hstack([y_train, y_train])

    # Scale
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_aug)
    X_test_scaled  = scaler.transform(X_test)

    weights = compute_class_weight(
        class_weight = "balanced",
        classes = np.unique(y_train_aug),
        y = y_train_aug
    )

    cw = dict(zip(np.unique(y_train_aug), weights))
    normal_label = label_encoder.transform(["normal"])[0]
    cw[normal_label] *= 1.5

    # Train
    print("\nTraining Random Forest...")
    rf = RandomForestClassifier(n_estimators=300,
                                class_weight = cw,
                                max_depth = None,
                                random_state=42,
                                min_samples_split = 2,
                                min_samples_leaf = 1
                                )
    rf.fit(X_train_scaled, y_train_aug)
    scores = cross_val_score(rf, X_train_scaled, y_train_aug, cv=5)
    print(scores, scores.mean())

    # Evaluate
    y_pred = rf.predict(X_test_scaled)
    proba = rf.predict_proba(X_test_scaled)
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)

    #Check Confusion Matrix
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

    # Show mapping
    print("\nMy Class Mapping (Save this for your Arduino code!):")
    for i, class_name in enumerate(label_encoder.classes_):
        print(f"ID {i} = {class_name}")

    # --- EXPORT TO C ---

    print("\nGenerating C Code...")
    # 1. Export Model
    c_code = port(rf, classname="RandomForest")
    with open('failure_classifier_rf.h', 'w') as f:
        f.write(c_code)

    # 2. Export Scaler Values (You need these in Arduino!)
    print(f"const float SCALER_MEAN[] = {{ {', '.join([f'{x:.6f}' for x in scaler.mean_])} }};")
    print(f"const float SCALER_SCALE[] = {{ {', '.join([f'{x:.6f}' for x in scaler.scale_])} }};")

    print("\nAll done! Download 'failure_classifier_rf.h' and use the Scaler arrays in your ESP32 code.")

pip install micromlgen